{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bvfUnAPAOMs",
        "outputId": "4e5c1e9c-e9e4-48b0-97d6-6b886a97978d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYRFsxmncxtB"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "8MA_OFn0dAFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"t5-small\"  # You can use a larger model like 't5-base' or 't5-large'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k41wfCNBc-Pt",
        "outputId": "031de320-f8ba-45ce-c975-01a259addb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Long PDF text example 1.\", \"Another long document text example.\"]\n",
        "summaries = [\"Summary of document 1.\", \"Summary of document 2.\"]"
      ],
      "metadata": {
        "id": "HcZnn2AcBn6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, texts, summaries, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.texts[idx]\n",
        "        target_text = self.summaries[idx]\n",
        "\n",
        "        # Tokenize inputs and targets\n",
        "        input_enc = self.tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "        target_enc = self.tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "\n",
        "        # Return as a dictionary\n",
        "        return {\n",
        "            \"input_ids\": input_enc[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_enc[\"input_ids\"].squeeze()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "g6VG9WATIIqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SummarizationDataset(texts, summaries, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "yumfaDrmI5bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n"
      ],
      "metadata": {
        "id": "hSdExnG0I94v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for batch in dataloader:\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "ldLd7Hm9JCvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kO_c_jzJLmb",
        "outputId": "c13c70a4-e62e-4472-b5c7-36a784d858bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get loss (cross-entropy)\n",
        "loss = outputs.loss"
      ],
      "metadata": {
        "id": "cwvr4aSYJQFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward pass and optimization\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3nU5TMjJZ-q",
        "outputId": "c7271fe4-2d97-45e1-eeb8-c27df2f7fd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 13.109818458557129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfKUfNZDJhbf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}